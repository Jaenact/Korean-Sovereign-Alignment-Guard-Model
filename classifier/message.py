import os
import json
import torch
import transformers
from transformers import AutoTokenizer, AutoModelForSequenceClassification  # í•„ìš” ì—†ìœ¼ë©´ ë‚˜ì¤‘ì— ì‚­ì œí•´ë„ ë¨

from Moderation_API import (
    get_guard_resources,
    classify_prompt as guard_classify_internal,
)

# =========================
# 0) Hugging Face í† í° ì„¤ì •
# =========================
HF_TOKEN = "ë°œê¸‰í•œ í† í°ìœ¼ë¡œ ë°”ê¾¸ê¸°"  # âš  ê¹ƒì— ì˜¬ë¦´ ë• ë¹¼ê¸°!

os.environ["HF_TOKEN"] = HF_TOKEN
os.environ["HF_HUB_TOKEN"] = HF_TOKEN
os.environ["HUGGINGFACEHUB_API_TOKEN"] = HF_TOKEN


# =========================
# 1) ì™œê³¡ íŒë³„ê¸° (ë‹¤ë¥¸ ì‚¬ëŒì´ ë§Œë“  ëª¨ë¸ì„ ë¶™ì´ëŠ” ìë¦¬)
# =========================
"""
ì—¬ê¸°ì„œëŠ” 'ì •ìƒ / ë¹„ì •ìƒ'ì„ íŒë³„í•˜ëŠ” ë¶„ë¥˜ê¸°ë¥¼ ë¡œë”©í•˜ê³ ,
ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ ë„£ì–´ ë¼ë²¨ê³¼ ì‹ ë¢°ë„ë¥¼ ëŒë ¤ì£¼ëŠ” ì¸í„°í˜ì´ìŠ¤ë§Œ ì •ì˜í•œë‹¤.

ë‹¤ë¥¸ ì‚¬ëŒì´ ë§Œë“  ë¶„ë¥˜ê¸° êµ¬ì¡°ì— ë§ê²Œ
- load_cls_model()
- classify_prompt()
ë‘ í•¨ìˆ˜ë§Œ êµ¬í˜„í•˜ë©´, ì•„ë˜ ìƒì„± íŒŒì´í”„ë¼ì¸ì€ ê·¸ëŒ€ë¡œ ì“¸ ìˆ˜ ìˆë‹¤.
"""


def load_cls_model():
    """
    ìš°ë¦¬ ìª½ ê°€ë“œ ëª¨ë¸(Moderation_API.py)ì„ ë¡œë”©í•˜ëŠ” ë¶€ë¶„.

    - get_guard_resources()ë¥¼ í†µí•´
      (nli_clf, fact_texts, fact_embs)ë¥¼ í•œ ë²ˆë§Œ ë©”ëª¨ë¦¬ì— ì˜¬ë¦°ë‹¤.
    - ì´ í•¨ìˆ˜ëŠ” (tokenizer, model) íŠœí”Œì„ ë¦¬í„´í•´ì•¼ í•˜ë¯€ë¡œ,
      í¸ì˜ìƒ tokenizer ìë¦¬ì— nli_clf, model ìë¦¬ì— (fact_texts, fact_embs)ë¥¼ ë„£ì–´ë‘”ë‹¤.
    """
    nli_clf, fact_texts, fact_embs = get_guard_resources()

    # ì•„ë˜ íŠœí”Œì€ main ìª½ì—ì„œ cls_tokenizer, cls_model ë³€ìˆ˜ë¡œ ë°›ê²Œ ë¨
    # cls_tokenizer â†’ nli_clf
    # cls_model     â†’ (fact_texts, fact_embs)
    return nli_clf, (fact_texts, fact_embs)


def classify_prompt(prompt: str, cls_tokenizer, cls_model):
    """
    ìš°ë¦¬ ê°€ë“œ ëª¨ë¸ì˜ classify_prompt()ë¥¼ ê°ì‹¸ì„œ
    - label: "ì •ìƒ" / "ë¹„ì •ìƒ"
    - confidence: float
    í˜•íƒœë¡œ ë°˜í™˜í•˜ëŠ” ì–´ëŒ‘í„° ì—­í• ì„ í•œë‹¤.

    cls_tokenizer: load_cls_model()ì—ì„œ ë„˜ê¸´ nli_clf
    cls_model    : load_cls_model()ì—ì„œ ë„˜ê¸´ (fact_texts, fact_embs)
    """

    nli_clf = cls_tokenizer
    fact_texts, fact_embs = cls_model

    # Moderation_API.classify_prompt() í˜¸ì¶œ
    res = guard_classify_internal(
        prompt,
        nli_clf=nli_clf,
        fact_texts=fact_texts,
        fact_embs=fact_embs,
    )

    # harmful ì—¬ë¶€ë¥¼ distortion ë¼ë²¨ë¡œ ë§¤í•‘
    if res.get("in_domain", False) and res.get("harmful", False):
        label = "ë¹„ì •ìƒ"
    else:
        # ë„ë©”ì¸ ë°–ì´ê±°ë‚˜ harmful=False ì¸ ê²½ìš° ëª¨ë‘ "ì •ìƒ"ìœ¼ë¡œ ì·¨ê¸‰
        label = "ì •ìƒ"

    # confidenceëŠ” CONTRADICT í™•ë¥  ê·¸ëŒ€ë¡œ ì‚¬ìš© (0~1)
    confidence = float(res.get("max_contradict_prob", 0.0))

    return label, confidence


# =========================
# 2) ìƒì„±ê¸° (Llama â€“ ì§ˆë¬¸ êµì • ì „ìš©)
# =========================

GEN_MODEL_ID = "meta-llama/Meta-Llama-3-8B-Instruct"

gen_pipe = transformers.pipeline(
    "text-generation",
    model=GEN_MODEL_ID,
    model_kwargs={"dtype": torch.bfloat16},  # torch_dtype ëŒ€ì‹  dtype
    device_map="auto",
    token=HF_TOKEN,
)

SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ í•œêµ­ ì—­ì‚¬, ë¬¸í™”, ì˜í† Â·ì „ìŸÂ·ì¸ê¶Œ ë¬¸ì œë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•´, "
    "í•œêµ­ì–´ë¡œ ì‘ì„±ëœ ì§ˆë¬¸ì„ ë‹¤ë“¬ëŠ” 'ì§ˆë¬¸ êµì •ê¸°'ì…ë‹ˆë‹¤.\n\n"
    "ë‹¹ì‹ ì˜ ì—­í• ì€ ì ˆëŒ€ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ 'ë‹µë³€'ì„ ìƒì„±í•˜ì§€ ë§ê³ , ì˜¤ì§ 'ì§ˆë¬¸'ë§Œ ë‹¤ì‹œ ì‘ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n"
    "ê°€ëŠ¥í•˜ë©´ ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§„ ì›ë˜ ì§ˆë¬¸ì˜ ë§íˆ¬ì™€ ì¢…ê²°ì²´(ë°˜ë§, ì¡´ëŒ“ë§, í•´ìš”ì²´, ~ëƒ?, ~ë‹ˆ? ë“±)ë¥¼ ìµœëŒ€í•œ ìœ ì§€í•˜ì„¸ìš”.\n"
    "ë‹¤ë§Œ, ì›ë˜ ì§ˆë¬¸ì´ ì§€ë‚˜ì¹˜ê²Œ ê³µê²©ì ì´ê±°ë‚˜ ëª¨ìš•ì ì¸ í‘œí˜„ì„ í¬í•¨í•˜ëŠ” ê²½ìš°ì—ëŠ”, "
    "ì „ë°˜ì ì¸ ë¬¸ì²´ëŠ” ìœ ì§€í•˜ë˜ í‘œí˜„ ìˆ˜ìœ„ë¥¼ ë‚®ì¶”ê³  ìˆœí™”ëœ í‘œí˜„ìœ¼ë¡œ ë°”ê¾¸ì‹­ì‹œì˜¤.\n\n"
    "ì…ë ¥ìœ¼ë¡œ distortion ê°’ê³¼ ì›ë˜ ì§ˆë¬¸(prompt)ì´ ì£¼ì–´ì§‘ë‹ˆë‹¤.\n"
    "distortionì´ 'ì •ìƒ'ì¸ ê²½ìš°:\n"
    "  - ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë¯¸ì™€ ë²”ìœ„ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œ, ë¬¸ì¥ì„ ë” ìì—°ìŠ¤ëŸ½ê³  ëª…í™•í•˜ê²Œ ë‹¤ë“¬ìœ¼ì„¸ìš”.\n"
    "  - ë¶ˆí•„ìš”í•˜ê²Œ ìƒˆë¡œìš´ ë…¼ì ì´ë‚˜ ì£¼ì œë¥¼ ì¶”ê°€í•˜ì§€ ë§ê³ , ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ë¬»ê³  ìˆëŠ” ë‚´ìš©ë§Œ ë‹¤ë£¨ë„ë¡ í•˜ì„¸ìš”.\n\n"
    "distortionì´ 'ë¹„ì •ìƒ'ì¸ ê²½ìš°:\n"
    "  - ì—­ì‚¬Â·ì‚¬ì‹¤ ì™œê³¡, ì‹ë¯¼ì§€ ì§€ë°°ë‚˜ ì „ìŸ ë²”ì£„ì˜ ë¯¸í™”, í”¼í•´ì ë¹„í•˜, í˜ì˜¤Â·í­ë ¥ í‘œí˜„ì´ ìˆë‹¤ë©´,\n"
    "    ê·¸ ë¶€ë¶„ë§Œ ë³´ë‹¤ ì‚¬ì‹¤ì— ê°€ê¹Œìš´ í‘œí˜„ê³¼ ì¤‘ë¦½ì ì¸ ì–´íœ˜ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.\n"
    "  - ê·¸ë˜ë„ ì‚¬ìš©ìê°€ ê¶ê¸ˆí•´í•˜ëŠ” í•µì‹¬ ì§ˆë¬¸(ë¬´ì—‡ì„ ì•Œê³  ì‹¶ì–´ í•˜ëŠ”ì§€)ì€ ìœ ì§€í•´ì•¼ í•˜ë©°, "
    "    ì§ˆë¬¸ì˜ ë²”ìœ„ë¥¼ ë¶ˆí•„ìš”í•˜ê²Œ ë„“íˆê±°ë‚˜ ìƒˆë¡œìš´ ì˜ì œë¥¼ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.\n"
    "  - íŠ¹ì • êµ­ê°€ë‚˜ ì§‘ë‹¨ ì „ì²´ë¥¼ ê°ì •ì ìœ¼ë¡œ ë¹„ë‚œí•˜ê¸°ë³´ë‹¤ëŠ”, 'ì–´ë–¤ ì…ì¥ì´ë‚˜ ë…¼ìŸì´ ìˆë‹¤'ë¼ëŠ” ì‹ìœ¼ë¡œ "
    "    ì§ˆë¬¸ì„ í‘œí˜„í•˜ë„ë¡ ì¡°ì •í•˜ì„¸ìš”.\n\n"
    "ì¤‘ìš”:\n"
    "  - ë‹¹ì‹ ì€ ì ˆëŒ€ ì§ˆë¬¸ì— ëŒ€í•œ 'ë‹µë³€'ì„ ì‘ì„±í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.\n"
    "  - ì˜¤ì§ êµì •ëœ ì§ˆë¬¸ í•˜ë‚˜ë§Œ ìƒì„±í•˜ê³ , ê·¸ ì§ˆë¬¸ì„ JSON í˜•ì‹ì˜ 'rewritten_prompt' í•„ë“œì— ë‹´ì•„ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.\n\n"
    "ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ë§Œ ì‚¬ìš©):\n"
    "{\"rewritten_prompt\": \"...\"}"
)


def generate_rewritten_prompt(prompt: str, distortion: str) -> str:
    """
    prompt    : ì›ë³¸ í”„ë¡¬í”„íŠ¸
    distortion: 'ì •ìƒ' ë˜ëŠ” 'ë¹„ì •ìƒ'
    â†’ LLMì´ ì™œê³¡ êµì •ì´ ë°˜ì˜ëœ 'êµì •ëœ ì§ˆë¬¸'ë§Œ ìƒì„±
    """

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {
            "role": "user",
            "content": (
                "ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸ì…ë‹ˆë‹¤.\n"
                "distortion ê°’ì— ë”°ë¼ ìœ„ ì§€ì¹¨ì„ ì ìš©í•˜ì—¬, ì§ˆë¬¸ì„ êµì •í•´ ì£¼ì„¸ìš”.\n\n"
                f'distortion: "{distortion}"\n'
                f'prompt: "{prompt}"\n\n'
                "ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì€ ì ˆëŒ€ ì‘ì„±í•˜ì§€ ë§ê³ , êµì •ëœ ì§ˆë¬¸ í•˜ë‚˜ë§Œ JSON í˜•ì‹ìœ¼ë¡œ ëŒë ¤ì£¼ì„¸ìš”.\n"
                "í˜•ì‹ ì˜ˆì‹œ: {\"rewritten_prompt\": \"...\"}"
            ),
        },
    ]

    terminators = [
        gen_pipe.tokenizer.eos_token_id,
        gen_pipe.tokenizer.convert_tokens_to_ids("<|eot_id|>"),
    ]

    outputs = gen_pipe(
        messages,
        max_new_tokens=256,
        eos_token_id=terminators,
        do_sample=False,
        temperature=0.0,
    )

    assistant_msg = outputs[0]["generated_text"][-1]["content"]

    # JSON ë¶€ë¶„ë§Œ íŒŒì‹±
    start = assistant_msg.find("{")
    end = assistant_msg.rfind("}")
    if start == -1 or end == -1:
        raise ValueError(f"JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n{assistant_msg}")

    json_str = assistant_msg[start : end + 1]
    data = json.loads(json_str)

    rewritten = data.get("rewritten_prompt", "").strip()
    return rewritten


# =========================
# 3) ì—”ë“œ íˆ¬ ì—”ë“œ íŒŒì´í”„ë¼ì¸ (ì§ˆë¬¸ êµì •ë§Œ)
# =========================

if __name__ == "__main__":
    # ğŸ”§ ë‹¤ë¥¸ ì‚¬ëŒì´ ë§Œë“  ë¶„ë¥˜ê¸° ë¡œë”© ë¶€ë¶„
    cls_tokenizer, cls_model = load_cls_model()
    print("Device set to use", gen_pipe.device)

    while True:
        user_prompt = input("\ní”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: q): ").strip()

        if user_prompt.lower() == "q":
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        if user_prompt == "":
            print("âš  ë¹ˆ í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
            continue

        # 1) ì™œê³¡ ì—¬ë¶€ íŒë³„
        distortion_label, conf = classify_prompt(user_prompt, cls_tokenizer, cls_model)
        print(f"\n[ë¶„ë¥˜ ê²°ê³¼] {distortion_label} (ì‹ ë¢°ë„: {conf:.3f})")

        print("â†’ LLMì´ êµì •ëœ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")

        # 2) êµì •ëœ ì§ˆë¬¸ ìƒì„± (ë‹µë³€ X)
        rewritten = generate_rewritten_prompt(user_prompt, distortion_label)

        print("\n=== êµì •ëœ ì§ˆë¬¸ ===")
        print(rewritten)
